services:
  # 0) Download GGUF into ./models (skips if file exists)
  model-fetch:
    image: alpine:3.20
    working_dir: /work
    env_file: .env
    command: >
      sh -lc '
        set -e;
        mkdir -p "${CUSTOM_MODEL_DIR}";
        cd "${CUSTOM_MODEL_DIR}";
        if [ ! -f "${CUSTOM_GGUF_FILE}" ]; then
          apk add --no-cache curl;
          echo "Downloading ${CUSTOM_GGUF_URL} -> ${CUSTOM_GGUF_FILE}";
          curl -fL "${CUSTOM_GGUF_URL}" -o "${CUSTOM_GGUF_FILE}";
        else
          echo "GGUF already present, skipping download.";
        fi
        ls -lh .
      '
    volumes:
      - ./:/work
    restart: "no"

  # 1) Register local model with Ollama using your Modelfile
  model-create:
    image: ollama/ollama:0.3.14
    env_file: .env
    depends_on:
      model-fetch:
        condition: service_completed_successfully
    entrypoint: ["sh","-lc"]
    command: >
      'ollama serve & sleep 2;
       ollama create "${CUSTOM_MODEL_TAG}" -f "/models/${CUSTOM_MODEL_DIR#models/}/Modelfile";
       kill %1 || true'
    environment:
      - OLLAMA_MODELS=/root/.ollama
      - OLLAMA_NO_GPU=1
    volumes:
      - ollama_models:/root/.ollama
      - ./models:/models
    restart: "no"

  # 2) Pre-pull the embedding model from the registry
  ollama-prepull-embed:
    image: ollama/ollama:0.3.14
    env_file: .env
    depends_on:
      model-create:
        condition: service_completed_successfully
    entrypoint: ["sh","-lc"]
    command: >
      'ollama serve & sleep 2;
       ollama pull "${OLLAMA_EMBED}";
       kill %1 || true'
    environment:
      - OLLAMA_MODELS=/root/.ollama
      - OLLAMA_NO_GPU=1
    volumes:
      - ollama_models:/root/.ollama
    restart: "no"

  # 3) Long-running Ollama server
  ollama:
    image: ollama/ollama:0.3.14
    depends_on:
      model-create:
        condition: service_completed_successfully
      ollama-prepull-embed:
        condition: service_completed_successfully
    environment:
      - OLLAMA_MODELS=/root/.ollama
      - OLLAMA_NO_GPU=1
    volumes:
      - ollama_models:/root/.ollama
      - ./models:/models:ro
    ports:
      - "11434:11434"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:11434/ >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 60

  # 4) Qdrant vector DB
  qdrant:
    image: qdrant/qdrant:v1.11.0
    restart: unless-stopped
    ports:
      - "6333:6333"
    volumes:
      - qdrant_storage:/qdrant/storage
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:6333/readyz >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 30

  # 5) Your FastAPI service
  api:
    build:
      context: ./api
      dockerfile: Dockerfile
    image: local/rag-api:0.1.0
    restart: unless-stopped
    env_file: .env
    depends_on:
      qdrant:
        condition: service_healthy
      ollama:
        condition: service_healthy
    ports:
      - "8080:8000"
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:8000/health/live >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 20

volumes:
  ollama_models:
  qdrant_storage: