version: "3.9"

services:
  qdrant:
    image: qdrant/qdrant:v1.11.0
    restart: unless-stopped
    ports: ["6333:6333"]
    volumes:
      - qdrant_storage:/qdrant/storage

  ollama-prepull:
    image: ollama/ollama:0.3.14
    entrypoint: ["sh","-lc"]
    command: >
      'ollama serve & sleep 2;
       ollama pull ${OLLAMA_LLM} || exit 1;
       ollama pull ${OLLAMA_EMBED} || exit 1;
       kill %1 || true'
    environment:
      - OLLAMA_NO_GPU=1
      - OLLAMA_MODELS=/root/.ollama
    volumes:
      - ollama_models:/root/.ollama
    restart: "no"

  ollama:
    image: ollama/ollama:0.3.14
    depends_on:
      ollama-prepull:
        condition: service_completed_successfully
    environment:
      - OLLAMA_NO_GPU=1
      - OLLAMA_MODELS=/root/.ollama
    volumes:
      - ollama_models:/root/.ollama
      - ./models:/models
    ports: ["11434:11434"]
    restart: unless-stopped            # drop if you have GPU support

  api:
    build: ./api
    restart: unless-stopped
    env_file: .env
    depends_on:
      - qdrant
      - ollama
    ports: ["8080:8000"]

volumes:
  qdrant_storage:
  ollama_models: